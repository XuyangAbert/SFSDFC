{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyMB66tCPh/t9uAQ83SE826T",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/XuyangAbert/SFSDFC/blob/main/example_sfsdfc.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!git clone https://github.com/XuyangAbert/SFSDFC.git"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ocXxgjIqLnah",
        "outputId": "73117875-f453-4561-d0a9-ea21ce426414"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Cloning into 'SFSDFC'...\n",
            "remote: Enumerating objects: 147, done.\u001b[K\n",
            "remote: Counting objects: 100% (142/142), done.\u001b[K\n",
            "remote: Compressing objects: 100% (103/103), done.\u001b[K\n",
            "remote: Total 147 (delta 69), reused 80 (delta 35), pack-reused 5\u001b[K\n",
            "Receiving objects: 100% (147/147), 548.30 KiB | 7.95 MiB/s, done.\n",
            "Resolving deltas: 100% (69/69), done.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import math\n",
        "import pandas as pd\n",
        "import numpy.matlib as b\n",
        "from sklearn.preprocessing import MinMaxScaler\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.neighbors import KNeighborsClassifier\n",
        "from sklearn.feature_selection import mutual_info_classif, mutual_info_regression\n",
        "from sklearn.metrics import f1_score\n",
        "import time\n",
        "from sklearn.model_selection import KFold\n",
        "from SFSDFC.entropy_estimators import *\n",
        "import multiprocessing as mp"
      ],
      "metadata": {
        "id": "6knQUMT2Loy7"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "metadata": {
        "id": "v0hVpGCWLYFP"
      },
      "outputs": [],
      "source": [
        "def Input():\n",
        "    # Read the data from the txt file\n",
        "    sample = pd.read_csv('/content/SFSDFC/Landcover.csv',header=None)\n",
        "    (N, L) = np.shape(sample)\n",
        "    dim = L - 1\n",
        "\n",
        "    label1 = sample.iloc[:,L-1]\n",
        "    label = label1.values\n",
        "    data = sample.iloc[:,0:dim]\n",
        "    # NewData = normalize(data)\n",
        "    NewData = Pre_Data(data)\n",
        "    return NewData,label\n",
        "\n",
        "def FeatureType(data):\n",
        "    [N,dim] = np.shape(data)\n",
        "    th = round(N**0.5)\n",
        "    F_cont = []\n",
        "    F_disc = []\n",
        "    for j in range(dim):\n",
        "        temp_unique = np.unique(data[:,j])\n",
        "        if len(temp_unique) > th:\n",
        "            F_cont.append(j)\n",
        "        else:\n",
        "            F_disc.append(j)\n",
        "    return F_cont, F_disc\n",
        "\n",
        "def Pre_Data(data):\n",
        "    [N,L] = np.shape(data)\n",
        "    scaler = MinMaxScaler()\n",
        "    scaler.fit(data)\n",
        "    NewData = scaler.transform(data)\n",
        "    return NewData\n",
        "\n",
        "def Distribution_Est(data, dim):\n",
        "    DC_mean = np.zeros(dim)\n",
        "    DC_std = np.zeros(dim)\n",
        "\n",
        "    for i in range(dim):\n",
        "        TempClass = data[:,i]\n",
        "        DC_mean[i] = np.mean(TempClass)\n",
        "        DC_std[i] = np.std(TempClass)\n",
        "\n",
        "    return DC_mean,DC_std\n",
        "\n",
        "def Feature_Dist1(DC_means,DC_std,data,Var,dim,Corr):\n",
        "\n",
        "    DisC = np.zeros((dim,dim))\n",
        "    Dist = []\n",
        "    for i in range(dim):\n",
        "        for j in range(i,dim):\n",
        "            DisC[i,j] = KLD_Cal(data,i,j,Var,Corr)\n",
        "            DisC[j,i] = DisC[i,j]\n",
        "            Dist.append(DisC[i,j])\n",
        "    return DisC,Dist\n",
        "\n",
        "def Feature_Dist2(data,dim):\n",
        "    Dist = []\n",
        "    DisC = np.zeros((dim,dim))\n",
        "\n",
        "    for i in range(dim):\n",
        "        for j in range(i,dim):\n",
        "            DisC[i,j] = Sym_Cal(data,i,j)\n",
        "            DisC[j,i] = DisC[i,j]\n",
        "            Dist.append(DisC[i,j])\n",
        "    return DisC,Dist\n",
        "\n",
        "def KLD_Cal(data,i,j,Var,Corr):\n",
        "    Var1 = Var[i]\n",
        "    Var2 = Var[j]\n",
        "\n",
        "    P = Corr[i,j]\n",
        "    Sim = Var1 + Var2 - ((Var1 + Var2)**2 - 4 * Var1 * Var2 * (1 - P**2))**0.5\n",
        "    D_KL = Sim / (Var1 + Var2)\n",
        "\n",
        "    return D_KL\n",
        "\n",
        "def Sym_Cal(data,i,j):\n",
        "    I_ij = midd(data[:,i],data[:,j])\n",
        "    H_I = entropyd(data[:,i])\n",
        "    H_J = entropyd(data[:,j])\n",
        "\n",
        "    if (H_I + H_J) == 0:\n",
        "        D_KL = 1\n",
        "    else:\n",
        "        D_KL = 1 - 2*(I_ij)/(H_I + H_J)\n",
        "    return D_KL\n",
        "\n",
        "def fitness_cal(DisC, DC_means, DC_std, data, StdF, gamma):\n",
        "    fitness = np.zeros(len(DC_means))\n",
        "    # print(np.shape(fitness))\n",
        "    for i in range(len(DC_means)):\n",
        "        TempSum = 0\n",
        "        for j in range(len(DC_means)):\n",
        "            if j != i:\n",
        "                D = DisC[i,j]\n",
        "                TempSum = TempSum + (math.exp(- (D**2) / StdF))**gamma\n",
        "        fitness[i] = TempSum\n",
        "    return fitness\n",
        "\n",
        "def Pseduo_Peaks1(DisC, Dist, DC_Mean, DC_Std, data, fitness, StdF, gamma, Var):\n",
        "\n",
        "    # The temporal sample space in terms of mean and standard deviation\n",
        "    sample = np.vstack((DC_Mean,DC_Std)).T\n",
        "\n",
        "    # Search Stage of Pseduo Clusters at the temporal sample space\n",
        "    NeiRad = 0.01*max(Dist) #0.01\n",
        "    # NeiRad = (StdF/gamma)\n",
        "    i = 0\n",
        "    marked = []\n",
        "    C_Indices = np.arange(1, len(DC_Mean)+1) # The pseduo Cluster label of features\n",
        "    PeakIndices = []\n",
        "    Pfitness = []\n",
        "    co = []\n",
        "    F = fitness\n",
        "    while True:\n",
        "\n",
        "        PeakIndices.append(np.argmax(F))\n",
        "        Pfitness.append(np.max(F))\n",
        "\n",
        "        indices = NeighborSearch1(DisC, data, sample, PeakIndices[i], marked, NeiRad, Var)\n",
        "\n",
        "        C_Indices[indices] = PeakIndices[i]\n",
        "        if len(indices) == 0:\n",
        "            indices=[PeakIndices[i]]\n",
        "\n",
        "        co.append(len(indices)) # Number of samples belong to the current\n",
        "    # identified pseduo cluster\n",
        "        marked = np.concatenate(([marked,indices]))\n",
        "\n",
        "        # Fitness Proportionate Sharing\n",
        "        F = Sharing(F, indices)\n",
        "\n",
        "        # Check whether all of samples has been assigned a pseduo cluster label\n",
        "        if np.sum(co) >= (len(F)):\n",
        "            break\n",
        "        i=i+1 # Expand the size of the pseduo cluster set by 1\n",
        "\n",
        "    C_Indices = Close_FCluster(PeakIndices,DisC,np.shape(DisC)[0])\n",
        "    return PeakIndices,Pfitness,C_Indices\n",
        "def Pseduo_Peaks2(DisC, Dist, DC_Mean, DC_Std, data, fitness, StdF, gamma):\n",
        "\n",
        "    # The temporal sample space in terms of mean and standard deviation\n",
        "    sample = np.vstack((DC_Mean,DC_Std)).T\n",
        "\n",
        "    # Search Stage of Pseduo Clusters at the temporal sample space\n",
        "#    NeiRad = 0.25 * StdF\n",
        "    NeiRad = 0.01*np.max(Dist)\n",
        "    # NeiRad = (StdF/gamma)\n",
        "    i = 0\n",
        "    marked = []\n",
        "    C_Indices = np.arange(1, len(DC_Mean)+1) # The pseduo Cluster label of features\n",
        "    PeakIndices = []\n",
        "    Pfitness = []\n",
        "    co = []\n",
        "    F = fitness\n",
        "    while True:\n",
        "        PeakIndices.append(np.argmax(F))\n",
        "        Pfitness.append(np.max(F))\n",
        "        indices = NeighborSearch2(DisC, data, sample, PeakIndices[i], marked, NeiRad)\n",
        "        C_Indices[indices] = PeakIndices[i]\n",
        "        if len(indices) == 0:\n",
        "            indices=[PeakIndices[i]]\n",
        "        co.append(len(indices)) # Number of samples belong to the current\n",
        "        # identified pseduo cluster\n",
        "        marked = np.concatenate(([marked,indices]))\n",
        "\n",
        "        # Fitness Proportionate Sharing\n",
        "        F = Sharing(F, indices)\n",
        "        # Check whether all of samples has been assigned a pseduo cluster label\n",
        "        if np.sum(co) >= (len(F)):\n",
        "            break\n",
        "        i=i+1 # Expand the size of the pseduo cluster set by 1\n",
        "    C_Indices = Close_FCluster(PeakIndices, DisC, np.shape(DisC)[0])\n",
        "    return PeakIndices,Pfitness,C_Indices\n",
        "def NeighborSearch1(DisC, data, sample, P_indice, marked, radius, Var):\n",
        "    Cluster = []\n",
        "    for i in range(np.shape(sample)[0]):\n",
        "        if i not in marked:\n",
        "            Dist = DisC[i, P_indice]\n",
        "            if Dist <= radius:\n",
        "                Cluster.append(i)\n",
        "    Indices = Cluster\n",
        "    return Indices\n",
        "\n",
        "def NeighborSearch2(DisC, data, sample, P_indice, marked, radius):\n",
        "    Cluster = []\n",
        "    for i in range(np.shape(sample)[0]):\n",
        "        if i not in marked:\n",
        "            Dist = DisC[i, P_indice]\n",
        "            if Dist <= radius:\n",
        "                Cluster.append(i)\n",
        "    Indices = Cluster\n",
        "    return Indices\n",
        "\n",
        "def Sharing(fitness, indices):\n",
        "    newfitness = fitness\n",
        "    sum1 = 0\n",
        "    for j in range(len(indices)):\n",
        "        sum1 = sum1 + fitness[indices[j]]\n",
        "    for th in range(len(indices)):\n",
        "            newfitness[indices[th]] = fitness[indices[th]] / (1+sum1)\n",
        "\n",
        "    return newfitness\n",
        "\n",
        "def Pseduo_Evolve(DisC, PeakIndices, PseDuoF, C_Indices, DC_Mean, DC_Std, data, fitness, StdF, gamma):\n",
        "\n",
        "    # Initialize the indices of Historical Pseduo Clusters and their fitness values\n",
        "    HistCluster = PeakIndices\n",
        "    HistClusterF = PseDuoF\n",
        "    while True:\n",
        "        # Call the merge function in each iteration\n",
        "        [Cluster,Cfitness,F_Indices] = Pseduo_Merge(DisC, HistCluster, HistClusterF, C_Indices, DC_Mean, DC_Std, data, fitness, StdF, gamma)\n",
        "        # Check for the stablization of clutser evolution and exit the loop\n",
        "        if len(np.unique(Cluster)) == len(np.unique(HistCluster)):\n",
        "            break\n",
        "\n",
        "        # Update the feature indices of historical pseduo feature clusters and\n",
        "        # their corresponding fitness values\n",
        "\n",
        "        HistCluster=Cluster\n",
        "        HistClusterF=Cfitness\n",
        "        C_Indices = F_Indices\n",
        "    # Compute final evolved feature cluster information\n",
        "    FCluster = np.unique(Cluster)\n",
        "    Ffitness = Cfitness\n",
        "    C_Indices = F_Indices\n",
        "\n",
        "    return FCluster, Ffitness, C_Indices\n",
        "#----------------------------------------------------------------------------------------------------------\n",
        "def Pseduo_Merge(DisC, PeakIndices, PseDuoF, C_Indices, DC_Mean, DC_Std, data, fitness, StdF, gamma):\n",
        "    if len(PeakIndices) == 1:\n",
        "        FCluster = PeakIndices\n",
        "        Ffitness = fitness[FCluster]\n",
        "        F_Indices = Close_FCluster(FCluster, DisC, np.shape(DisC)[0])\n",
        "        return FCluster, Ffitness, F_Indices\n",
        "    # Initialize the pseduo feature clusters lables for all features\n",
        "    F_Indices = C_Indices\n",
        "    # Initialize the temporal sample space for feature means and stds\n",
        "    sample = np.vstack((DC_Mean,DC_Std)).T\n",
        "    ML = [] # Initialize the merge list as empty\n",
        "    marked = [] #List of checked Pseduo Clusters Indices\n",
        "    Unmarked = [] # List of unmerged Pseduo Clusters Indices\n",
        "    for i in range(len(PeakIndices)):\n",
        "            M = 1 # Set the merge flag as default zero\n",
        "            MinDist = math.inf # Set the default Minimum distance between two feature clusters as infinite\n",
        "            MinIndice = -1 # Set the default Neighboring feature cluster indices as zero\n",
        "            # Check the current Pseduo Feature Cluster has been evaluated or not\n",
        "            if PeakIndices[i] not in marked:\n",
        "                for j in range(len(PeakIndices)):\n",
        "                        if j != i:\n",
        "                            # Divergence Calculation between two pseduo feature clusters\n",
        "                            D = DisC[PeakIndices[i], PeakIndices[j]]\n",
        "                            if MinDist > D:\n",
        "                                MinDist = D\n",
        "                                MinIndice = j\n",
        "                if MinIndice >= 0:\n",
        "                    # Current feature pseduo cluster under check\n",
        "                    Current = sample[PeakIndices[i],:]\n",
        "                    CurrentFit = PseDuoF[i]\n",
        "                    # Neighboring feature pseduo cluster of the current checked cluster\n",
        "                    Neighbor = sample[PeakIndices[MinIndice],:]\n",
        "                    NeighborFit = PseDuoF[MinIndice]\n",
        "\n",
        "                    # A function to identify the bounady feature instance between two\n",
        "                    # neighboring pseduo feature clusters\n",
        "                    BP=Boundary_Points(DisC, F_Indices,data, PeakIndices[i], PeakIndices[MinIndice])\n",
        "                    BPF=fitness[BP]\n",
        "                    if BPF<1*min(CurrentFit,NeighborFit):\n",
        "                        M=0 # Change the Merge flag\n",
        "\n",
        "                    if M == 1:\n",
        "                        ML.append([PeakIndices[i],PeakIndices[MinIndice]])\n",
        "                        marked.append(PeakIndices[i])\n",
        "                        marked.append(PeakIndices[MinIndice])\n",
        "                    else:\n",
        "                        Unmarked.append(PeakIndices[i])\n",
        "    NewPI = []\n",
        "    # Update the pseduo feature clusters list with the obtained mergelist\n",
        "    for m in range(np.shape(ML)[0]):\n",
        "        # print(ML[m][0],ML[m][1])\n",
        "        if fitness[ML[m][0]] > fitness[ML[m][1]]:\n",
        "            NewPI.append(ML[m][0])\n",
        "            F_Indices[C_Indices==ML[m][1]] = ML[m][0]\n",
        "        else:\n",
        "            NewPI.append(ML[m][1])\n",
        "            F_Indices[C_Indices==ML[m][0]] = ML[m][1]\n",
        "    # Update the pseduo feature clusters list with pseduo clusters that have not appeared in the merge list\n",
        "    for n in range(len(PeakIndices)):\n",
        "        if PeakIndices[n] in Unmarked:\n",
        "            NewPI.append(PeakIndices[n])\n",
        "\n",
        "    # Updated pseduo feature clusters information after merging\n",
        "    FCluster = np.unique(NewPI)\n",
        "    FCluster = FCluster.astype(int)\n",
        "    Ffitness = fitness[FCluster]\n",
        "    F_Indices = Close_FCluster(FCluster, DisC, np.shape(DisC)[0])\n",
        "    return FCluster, Ffitness, F_Indices\n",
        "\n",
        "def Boundary_Points(DisC, F_Indices, data, Current, Neighbor):\n",
        "\n",
        "    [N, dim] = np.shape(data)\n",
        "    TempCluster1 = np.where(F_Indices == Current)\n",
        "    TempCluster2 = np.where(F_Indices == Neighbor)\n",
        "\n",
        "    TempCluster = np.append(TempCluster1,TempCluster2)\n",
        "    D = []\n",
        "#    D = np.inf\n",
        "#    FI = Current\n",
        "#    print(len(TempCluster))\n",
        "    for i in range(len(TempCluster)):\n",
        "        D1 = DisC[TempCluster[i], Current]\n",
        "        D2 = DisC[TempCluster[i], Neighbor]\n",
        "#        if D < abs(D1-D2):\n",
        "#            D = abs(D1-D2)\n",
        "#            FI = i\n",
        "        D.append(abs(D1 - D2))\n",
        "    if not D:\n",
        "        BD = Current\n",
        "    else:\n",
        "        FI = np.argmin(D)\n",
        "        BD = TempCluster[FI]\n",
        "\n",
        "    return BD\n",
        "\n",
        "def PseduoGeneration(PseP,N):\n",
        "\n",
        "    Pse_Mean = PseP[:,0]\n",
        "    Pse_Std = PseP[:,1]\n",
        "\n",
        "    # Data = (np.zeros((N,len(Pse_Mean))))\n",
        "\n",
        "    Data = np.zeros((N,len(Pse_Mean)))\n",
        "\n",
        "    for i in range(len(Pse_Mean)):\n",
        "\n",
        "        Data[:, i] = (np.repeat(Pse_Mean[i],N) + Pse_Std[i] * np.random.randn(N)).T\n",
        "\n",
        "    return Data\n",
        "\n",
        "def Psefitness_cal( PseP, sample, data, PseduoData, StdF, gamma):\n",
        "    OriFN = np.shape(sample)[0]\n",
        "    PN = np.shape(PseP)[0]\n",
        "    PsePF = np.zeros(PN)\n",
        "    for i in range(PN):\n",
        "        TempSum = 0\n",
        "        for j in range(OriFN):\n",
        "            Var1 = np.var(data[:,j])\n",
        "            Var2 = np.var(PseduoData[:,i])\n",
        "            P = np.corrcoef(data[:,j],PseduoData[:,i])[0,1]\n",
        "            Sim = Var1 + Var2 - ((Var1 + Var2)**2 - 4 * Var1 * Var2 * (1 - P**2))**0.5\n",
        "            D_KL = Sim / (Var1 + Var2)\n",
        "            TempSum = TempSum + (math.exp(-(D_KL**2)/StdF))**gamma\n",
        "        PsePF[i] = TempSum\n",
        "    return PsePF\n",
        "\n",
        "def Close_FCluster(FCluster,DisC,dim):\n",
        "    F_Indices = np.arange(dim)\n",
        "    for i in range(dim):\n",
        "        dist_fcluster = DisC[i,FCluster]\n",
        "        F_Indices[i] = FCluster[np.argmin(dist_fcluster)]\n",
        "    return F_Indices\n",
        "\n",
        "def ContinousFeatures(data,label,f_cont):\n",
        "    [N, dim] = np.shape(data)\n",
        "    if len(f_cont) < 1:\n",
        "        return []\n",
        "    if len(f_cont) == 1:\n",
        "        return f_cont\n",
        "    contin_sample = data[:,f_cont]\n",
        "    [N1, dim1] = np.shape(contin_sample)\n",
        "    [DC_means1, DC_std1] = Distribution_Est(contin_sample,dim1)\n",
        "    Var1 = np.var(contin_sample,axis=0)\n",
        "    Corr1 = np.corrcoef(contin_sample.T)\n",
        "    DisC1,Dist1 =  Feature_Dist1(DC_means1,DC_std1,contin_sample,Var1,dim1,Corr1)\n",
        "    StdF1 = (np.mean(np.power(Dist1,0.5)))**2\n",
        "    gamma1 = 5\n",
        "\n",
        "    fitness1 = fitness_cal(DisC1, DC_means1, DC_std1, contin_sample, StdF1, gamma1)\n",
        "    oldfitness1 = np.copy(fitness1)\n",
        "    [PeakIndices1,Pfitness1,C_Indices1] = Pseduo_Peaks1(DisC1, Dist1, DC_means1,\n",
        "    DC_std1,contin_sample,fitness1,StdF1,gamma1, Var1)\n",
        "\n",
        "    fitness1 = oldfitness1\n",
        "    # Pseduo Clusters Infomormation Extraction\n",
        "    PseDuo1 = DC_means1[PeakIndices1] # Pseduo Feature Cluster centers\n",
        "    PseDuoF1 = Pfitness1 # Pseduo Feature Clusters fitness values\n",
        "    #-------------Check for possible merges among pseduo clusters-----------#\n",
        "    [FCluster1,Ffitness1,C_Indices1] = Pseduo_Evolve(DisC1, PeakIndices1,\n",
        "    PseDuoF1, C_Indices1, DC_means1, DC_std1, contin_sample, fitness1, StdF1, gamma1)\n",
        "\n",
        "    SF1 = []\n",
        "\n",
        "    label = label.reshape(N,)\n",
        "\n",
        "    C_Indices1 = Close_FCluster(FCluster1,DisC1,dim1)\n",
        "\n",
        "    for i in FCluster1:\n",
        "        tempf_cluster1 = np.where(C_Indices1==i)[0]\n",
        "        if len(tempf_cluster1) > 1:\n",
        "            temp_fea1 = data[:,tempf_cluster1]\n",
        "            f_rel1 = mutual_info_classif(temp_fea1,label)\n",
        "            SF1.append(tempf_cluster1[np.argmax(f_rel1)])\n",
        "        else:\n",
        "            SF1.append(i)\n",
        "    return f_cont[SF1]\n",
        "\n",
        "def DiscreteFeatures(data,label,f_disc):\n",
        "    [N, dim] = np.shape(data)\n",
        "    if len(f_disc) < 1:\n",
        "        return []\n",
        "    disct_sample = data[:,f_disc]\n",
        "    [N2, dim2] = np.shape(disct_sample)\n",
        "    [DC_means2, DC_std2] = Distribution_Est(disct_sample,dim2)\n",
        "\n",
        "    DisC2,Dist2 =  Feature_Dist2(disct_sample,dim2)\n",
        "    StdF2 = max(Dist2)\n",
        "    gamma2 = 5\n",
        "\n",
        "    fitness2 = fitness_cal(DisC2, DC_means2, DC_std2, disct_sample, StdF2, gamma2)\n",
        "    oldfitness2 = np.copy(fitness2)\n",
        "    [PeakIndices2,Pfitness2,C_Indices2] = Pseduo_Peaks2(DisC2, Dist2, DC_means2,\n",
        "    DC_std2,disct_sample,fitness2,StdF2,gamma2)\n",
        "\n",
        "    fitness2 = oldfitness2\n",
        "    # Pseduo Clusters Infomormation Extraction\n",
        "    PseDuo2 = DC_means2[PeakIndices2] # Pseduo Feature Cluster centers\n",
        "    PseDuoF2 = Pfitness2 # Pseduo Feature Clusters fitness values\n",
        "    #-------------Check for possible merges among pseduo clusters-----------#\n",
        "    [FCluster2,Ffitness2,C_Indices2] = Pseduo_Evolve(DisC2, PeakIndices2,\n",
        "    PseDuoF2, C_Indices2, DC_means2, DC_std2, disct_sample, fitness2, StdF2, gamma2)\n",
        "\n",
        "    SF2 = []\n",
        "\n",
        "    label = label.reshape(N,)\n",
        "\n",
        "    C_Indices2 = Close_FCluster(FCluster2,DisC2,dim2)\n",
        "\n",
        "    for i in FCluster2:\n",
        "        tempf_cluster2 = np.where(C_Indices2==i)[0]\n",
        "        if len(tempf_cluster2) > 1:\n",
        "            temp_fea2 = data[:,tempf_cluster2]\n",
        "            f_rel2 = mutual_info_classif(temp_fea2,label)\n",
        "            SF2.append(tempf_cluster2[np.argmax(f_rel2)])\n",
        "        else:\n",
        "            SF2.append(i)\n",
        "    return f_disc[SF2]\n",
        "\n",
        "def feature_sel_fun(data, label, f_cont, f_disc):\n",
        "  [N, dim] = np.shape(data)\n",
        "  SF1 = ContinousFeatures(data,label,f_cont)\n",
        "  SF2 = DiscreteFeatures(data,label,f_disc)\n",
        "\n",
        "  if len(SF2) > 0 and len(SF1) > 0:\n",
        "      SF = np.concatenate([SF1,SF2])\n",
        "  elif len(SF1) > 0:\n",
        "      SF = SF1\n",
        "  else:\n",
        "      SF = SF2\n",
        "  return SF\n",
        "#--------------------------------------------------------------------------------------------------------------\n",
        "\n",
        "def calculate_accuracy(data, test_x, label, test_y, f_cont, f_disc):\n",
        "    SF = feature_sel_fun(data, label, f_cont, f_disc)\n",
        "    [N, dim] = np.shape(data)\n",
        "    true_label = label.reshape(N,)\n",
        "\n",
        "    clf1 = KNeighborsClassifier(n_neighbors=3)\n",
        "    clf2 = KNeighborsClassifier(n_neighbors=3)\n",
        "\n",
        "    clf1 = clf1.fit(data[:,SF],true_label)\n",
        "    clf2 = clf2.fit(data,true_label)\n",
        "    acc1 = clf1.score(test_x[:,SF],test_y)\n",
        "    acc2 = clf2.score(test_x,test_y)\n",
        "    return (acc1, acc2, len(SF))\n",
        "\n",
        "def aggregate_accuracy(acc):\n",
        "    global Acc1, Acc2, selected_feature\n",
        "    Acc1.append(acc[0])\n",
        "    Acc2.append(acc[1])\n",
        "    selected_feature = acc[2]"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "if __name__ == '__main__':\n",
        "    start = time.time()\n",
        "    [data1,label1] = Input()\n",
        "\n",
        "    f_cont, f_disc = FeatureType(data1)\n",
        "\n",
        "    f_cont = np.asarray(f_cont)\n",
        "    f_disc = np.asarray(f_disc)\n",
        "\n",
        "    kf = KFold(n_splits=10,shuffle=True)\n",
        "    X = data1\n",
        "    Acc1 = []\n",
        "    Acc2 = []\n",
        "    selected_feature = 0\n",
        "\n",
        "    pool = mp.Pool(mp.cpu_count())\n",
        "\n",
        "    for train_index, test_index in kf.split(X):\n",
        "        data, test_x = X[train_index], X[test_index]\n",
        "        label, test_y = label1[train_index], label1[test_index]\n",
        "        pool.apply_async(calculate_accuracy,\n",
        "                         args=(data, test_x, label, test_y, f_cont, f_disc),\n",
        "                         callback=aggregate_accuracy)\n",
        "    pool.close()\n",
        "    pool.join()\n",
        "\n",
        "    print(\"Cross-validated accuracy 1: \", np.mean(Acc1))\n",
        "    print(\"Cross-validated accuracy 2: \", np.mean(Acc2))\n",
        "\n",
        "    print(\"Number of Selected Features: \", selected_feature)\n",
        "    end = time.time()\n",
        "    print('The total time in seconds:',end-start)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "QrDyS6cLMpja",
        "outputId": "47134a2a-2e85-49b7-9576-27fab74ab5f9"
      },
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Cross-validated accuracy 1:  0.782089552238806\n",
            "Cross-validated accuracy 2:  0.7850965759438103\n",
            "Number of Selected Features:  57\n",
            "The total time in seconds: 8.722939252853394\n"
          ]
        }
      ]
    }
  ]
}
